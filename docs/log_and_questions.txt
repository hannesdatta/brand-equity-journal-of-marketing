26 January 2016
- Availability of brands over time / cut-off (or: SUR with unequal equation length, still, dummies may not be identified well (e.g., attraction decreases just because a new brand enters the market)
--> we decide to model only the longest consecutive stretch of observations for each brand, given that we have at least four years of data
- What are attribute variables used for at this stage?

27 January 2016
- what are the exact variable names that should go into the model

	 brand_name 			used as brand_id
	 Brand_ID 				(column needed to "carry along" in my code?)
	 
	 act_pr_bt 				  orig paper
	 reg_pr_bt     			                  		1)      --> regular price has to be in there for sure
	 pi_bt 					? 						2)   a) --> this for first round; reg. and price index
	 newprdum_bt      
	 fd_bt 					? 
	 pi_unsup_bt 			?							 b)  
	 pi_sup_bt    			?							 b)
	 fd_wo_bt   			?						     b)
	 promo_bt 				? orig paper
	 
	 pct_store_skus 								3)      --> take this as %-SKUs in category are of our brand
	 distrwidth_bt 			? 
	 LineLength_bt			x
	 distrdepth_bt 			?
	 advertising_bt			x                       4)  2nd round
	 adv_stock				  orig paper                preliminary
	 
	 
	 
	 ATTRIBUTES				(I need to clarify again which are the columns to take into account.
							(- Have we already dropped one of the attribute columns?)
							(- Which columns make up one attribute?)
							(- Maybe consider relabeling these columns)
	 
	 packet_bt 
	 liquid_bt  
	 granul_bt 
	 otherfrm_bt 
	 aspartame_bt 
	 sucralose_bt
	 agave_bt 
	 saccharin_bt 
	 othersugar_typ_bt 
	 
	 --> All possibilities are there.
	 --> I get the look-up tables
	 
	 
- Calculation of marketing elasticities:
  beta * X * (1- share)
  
  where share = avg. market share of a brand
  X = AVERAGE marketing of (a) that brand, (b) across all brands
  
  Question: Which marketing mix levels to use? (a) or (b) or ...?
  
  --> Kusum: do option (a)
  
  --> Standardize intercepts
  --> estimate it for every year, drop alphas
  
- Suggestion for an updated Table 4 to reflect our new model:

Columns: Marketing Mix Parameter | Weighted Elasticity | Median Elasticity |  | % brands < 0 | % brands > 0

Table notes: Meta-analytic Z | Significance of Meta-analytic Z.  Weights equal to the elasticities' inverse standard error.


- Are more data sets available already?
- Second stage: Correlation with BAV; could you point me to the data?      KUSUM WILL PROVIDE THE DATA
- Any mean-centering to be used? Currently I do not center any variables.  NO
- Notes: The model document has been updated to reflect our discussion from 26 January 2016.
- Suggestion for next steps:
	o implement copulas
	o calculate elasticities
	o develop maximum likelihood estimator to enable HLM-type models and estimation of advertising carry-over
	o calculate VIFs (do we care much?) --> always check (but, if signs are ok... we are fine)


- Elasticities
- Face validity


2 February 2016
----------------
MY AGENDA

- What I did:
  o Implemented model, as layed out in the \analysis_hannes\docs folder (@Harald, please have a look at it)
  o Plotted all data (--> some questions still, see below), output to analysis_hannes\derived\audit\...
  o Face validity of coefficients not given (problematic are regular price), also see issues below
  o Recoded pi variable to discount variable (= 1 - pi_bt), easier to interpret (agree?)

Questions:
- Harald: any comment on the model document?
  o Let us also discuss formula (8). I'm not sure where we want the second stage variables to enter. I suppose only for year 1? --> we corrected the document.
  
- Can you please explain the differences between the columns brand_name, Brand_ID, and brand_name_recode? (which ones are crucial for me to "carry along"?
	o Answer: Brand_ID --> BAV (BAV data)
	o brand_name_recode --> Full name of the brand (shorter version)
	
- The price variables contain problematic outliers, causing coefficient to lose face validity. See plots in \derived\audit\variables_by_brand\, e.g., for the sugarsub category. 
	o Hannes: will prepare a list with brands and dates, and send it to Kusum/Rong to verify.
	
- Variables: pct_store_skus vs. LineLength_bt? Let's discuss benefits and drawbacks
	o distribution variable: breadth and depths were standard (but multicollinear); breadth did not have much variation.
	o  pct_store_skus is a variant of total distribution, where we hope to capture both breadth and depth. We continue with this metric.

- Initialization period for adstock and regular price: drop first four weeks of the data.
	o right now kick out first four weeks, until further notification.
	
- When do we kick out variables from estimation (sometimes line length only contains 2 or 3 values). Also, ad stock sometimes has 1 value only.
	o Leave short time series in there (significance will "punish" these variables anyways).
	o Kick out variables with only one observation
	
- apply a minimum-market share rule by year? (e.g., see soft n gentle in toilet tissue)
	
  o at least 1% in at least four consective years --> to be verified with Kusum and Harald.

- 90% market share rule already applied? (some brands appear very small)
	
  o overview about available brands andd market shares: lapply(datasets, function(x) x[, list(ssales=sum(sales_bt)), by=c('brand_name')][, list(ms=100*(ssales/sum(ssales)))])

Meeting summary:
- Harald will provide BLS data for deflation of prices
- Kusum will 
  o verify lagged weights for re-weighing (in cases where we do not have enough previous observations)
  o check with Rong regarding peaks and drops in prices (see above); Hannes will send some plots.
- Hannes will
  o apply the brand selection rule and report back to Harald and Kusum ("at least m% marketshare for at least y consecutive years").
  o integrate BLS data for deflation
  o rerun models and verify elasticities
  
  o model development
    x implement copulas
    x develop maximum likelihood estimator to enable HLM-type models and estimation of advertising carry-over
    x calculate VIFs
  
  
3 February 2016
===============

What I have done:

- Deflated act_pr_bt, reg_pr_bt, and rev_bt (I slightly had to change category mappings, compared to what Harald suggested). 
- Updated model document (replaced j by b').

Questions/Issues:
- deflate advertising spending?
- I noticed that the mapping of weeks to years is slightly incorrect, e.g., the starting week of 1322 (2004-12-27) is mapped to 01-2005, and not 12-2004. The problem is a previous underlying assumption that years always have 52 weeks. However, this is not correct. The years 2004 and 2009 have 53 weeks. I have corrected all years acoordingly: I assign that year for which most dates fall in the given year (in this case, 5 days fall in that year).
(The recoding was needed to merge CPI data).
- can I move the category data folders to a joint subfolder \Data? (makes directory structure easier)


- By the way we define elasticities, brands with small market shares have HIGH elasticities beta_b * (1 - marketshare_bt) * mean(X_bt). Is that really a desirable property? (cf. elasticity SUGAR TWIN in sugar subs, small brands, but weighs heavily, so does SWEETMATE).

4 February 2016
===============

- Rong will process remaining categories

- Model development
	o Simulate data, retrieve parameters (also SEs)
		x Make homogenous model first, without attributes
		x Homogenous model with attributes
		x Differential model
	
	o Learn about Geometric vs. Base-brand approach (and parameter recovery)
	
	o Estimate model on real sugarsub data
	o Code up VIFs
	
	(Final goal: intercepts driven by CBBE)
	
	o Attributes? Do they mess things up?
	o Inclusion CBBE metrics 

- Integrate all data, define brand selection criteria


8 February 2016 + Solving these issues on 9 February
==================
- Done: 
	- Integrated beer category; plots look good.
	- Simulated homogenous and differential models, without attributes. Works (except the intercepts are off)
	- VIFs are gigantic (even without having product attributes in the model); 
	  let us re-define which variables we'd like to be in the test equations (e.g., brand-year dummies?).
	  Alternatively, let us consider 'mimicking' how VIFs are in a base-brand approach. They should be much lower.
	- Price coefficients for sugarsubs (without attributes) is still not face valid.
	
- Pending: 
	- Geometric vs. Base-brand approach & parameter recovery CONFIRMED: it is the same thing.
	- Brand intercepts not well recovered SOLVED
	- Standard errors slightly diverge between SUR (systemfit in R), and my own code (magnitude negligible though) SOLVED
	- Recovery of error term still to be verified SOLVED
	- Estimation of beer category not possible yet --> will be done with base brand approach.
	
11 February 2016
================

- Done:
  o Verified simulation code; wrote estimation procedure for differential effects MNL attraction model
  o Elasticities look face-valid
    
- Questions:
  o identification of year dummies for benchmark brand
  o clarify pz_dn and rz_bl versus pizza_dinner and razor_blade SOLVED
  o how to report sales response estimates= (table 4): mean of weighted means in a category, or weighted means across all categories
  o adv. in beer & laundry detergents
  
- Pending: 
  o verify parameter estimate changes for models with and without attributes
  o report VIFS: model with and without attributes

  o try to back out alpha_B
  o replicate CBBE correlation analysis with final model results on all categories

  o bring in CBBE
  o add Gaussian Copulas

  
15 February 2016
=================
 
- Data
  o Tying together of data sets from different years continues to cause spikes in pct_store_
  o Ad_stock_bt variable causes duplicates in toothpa category; for now excluded
  
- Estimation
  o beer, laundet, toitisu: advertising coefficients can't be estimated; any experience in these categories?

- To be discussed
  o We have extreme VIFs
  o CBBE
     o taking a_by as direct estimates of brand equity seems invalid (exp(a_by) seems more appropriate).
	 o But: only difference to benchmark brand is recovered
     o Solution: similar to round 1, we can simulate market shares at average marketing mix.
	 o Alternatives: residuals?
  o Let's discuss how CBBE enters our analysis
  o Comment by reviewer: why no autoregressive component?
  o Attributes: broader attribute definitions?
 
17 February 2016
=================

- We decide to run the following models:
  (a) xvars_heterog=c('promo_bt', 'ract_pr_bt', 'pct_store_skus_bt', 'adstock_bt')
  (b) xvars_heterog=c('pi_bt', 'rreg_pr_bt', 'pct_store_skus_bt', 'adstock_bt') * this is the "minimum" model we need *
  (c) xvars_heterog=c('fd_bt', 'pi_bt', 'rreg_pr_bt', 'pct_store_skus_bt', 'adstock_bt') *** this is possibly the final model ***
  (d) xvars_heterog=xvars_heterog=c("pi_unsup_bt" ,"pi_sup_bt", 'fd_wo_bt', 'rreg_pr_bt', 'pct_store_skus_bt', 'adstock_bt') * only if face valid and estimatable consistently *
 
 
19 February 2016
=================

- Implemented Praise-Winsten correction for auto-correlation in marketingtool's itersur()
- Implemented computation of SBBE (normalization proposed by Dennis Fok, email from 16 February 2016); computation of standard errors using the Delta Method
- Implemented merging with CBBE, and computation of correlations
- Implemented endogeneity corrections using Gaussian Copulas
- Tried out estimation using different variable combinations (see notes above from 17 Feb. 2016)

Pending:
- a few exceptions in estimation laundet, toitisu and beer (e.g., changing benchmark brand, removing adstock).


24 February 2016
=================
- Solved advertising estimation problems by rescaling
- Correlations between elasticities (comment by R2); Harald: not relevant to test anymore, given the new model
- Test for seasonality in marketshares
  o Have a look at the marketshare plots
  o Alternatively, I could run an ANOVA with 3 quarterly dummies; check p-value of ANOVA
- Factor analysis for BAV: to be seen
- HLM versus second-stage weighted least squares
  --> we settle on a second-stage meta analysis
  
To do:
------

- Data:
  o Kusum/Rong: verify pct_store_sku variable
  o Hannes: add ketchup, incorporate BAV brand deletions, apply new selection rule ("retain all BAV brands")

- Model
  o Verify copula significance and normality of untransformed variables (DONE)
  o Implement advertising decay / grid search
  o Rerun on new data
  o Output results as summary CSV file with SBBE / other measures
  
- Meta analysis of 
  o correlations ("explore" heterogeneity")
  o marketing elasticities
  o variables in second stage
    x brand-level variables
	  - market leader vs. follower OR market share
	  - price positioning (average price, rescaled between -1 and 1)
	  - subbrand or major brand
	  - advertising intensity
	x category-level characteristics
	  - often vs. rarely bought products
	  - degree of competition
	  - foot vs. non-food
  o other
    x what about the "non"-correlating cases: is this predictive of performing bad in the future? "tracking" in the danger zone
      -> when do the metrics match, and when not?
    x cost implications
    x lagged CBBE effects

  o Model specs
    x clustered standard errors
	x check VIFs in second stage, if necessary, run factor analysis
	
15 March 2016
===================

Estimated models:
- Herfindahl / c4 / cat sales growth / brand sales growth / light dummy
- Elasticities vs. coefficients

To do:
- Re-compute growth (last year sales / first year sales) ^ (1/10) (where 10 is the number of years); do it for absolute and relative
- recompute secondary category dummy (final brand coding\allcat_brand_revenue_coded.xlsx)
- plots for category sales
- MCI vs. MNL

x standardize 
x no category main effects... also for continuous variables, also for c4.
x interactions to be mean-centered.
x WLS...? clustered standard errors...? / Random effects or clustered... / ...
x lagged CBBE in SBBE regressions may cause too much MC
x 

18 March 2016
=============

ADSTOCK
=======

We need to select for which brands adstock enters the model. Previously, we have settled on "non-zero advertising spending for at least 8 weeks". 
What I actually implemented was: non-zero ADSTOCK for at least 8 weeks, measured at a precision of six digits after the comma. 

Now I realize that the precision has been a black box, because I did not know in what multiples advertising was actually measured. 
Next to that, it's weird to write it like that. Today, I learnt advertising is measured in multiples of ,000. Given that we have a lot of small brands with very little spending, 
I suggest to openly say that we round to multiples of 1 USD.

This is how I would write it up:
 "[...] where adstock_bt is the exponentially smoothed advertising spending of brand b in period t, 
 expressed in increments of 1US$. The smoothing parameter lambda is estimated from the data. We do not include adstock if it exceeds 1 USD in less than 8 weeks."


SCALING OF VARIABLES
====================

In previous versions of the code, I rescaled the variables to be between 0 and 1, and then applied the transformations necessary for an attraction model. 
When implementing the MCI model, and getting some advice by Kusum, I realize that this transformation is incorrect. I have not fully digested WHY, but I can see it in simulations.

What I do now is: 
 (1) First transform the system of equations to its corresponding MCI or MNL attraction model
 (2) Then apply a rescaling (notably needed for advertising)
 (3) Estimate the model via SUR
 (4) Transform the coefficients and variance-covariance matrix, as if the model had been estimated without rescaling
 (5) Compute elasticities

ATTRIBUTES IN AN MCI MODEL
==========================
Our attribute variables are measured between 0 and 1. I now multiply this metric by 100, such that "adding 1" (to prevent taking the log of 0) has a negligible impact. How we write it up:

"[...] where attribute_kbt indicates how many SKUs of brand b in period t are of attribute k (measured in percentage points)."

When estimating the MCI model, we could further add:

"We add 1 to all variables that can take on zero values".

HHCLEAN
========
Just to keep it on the agenda, we need to decide what to do with - notably - the HHCLEAN category (sales spike in category sales in 2007). I understand that the shop selection is driving this spike.


30 March 2016
==================

Low correlations: verify correctness of MCI/MNL; convert intercepts to marketshares
Adstock: "include only if advertising is non-zero for at least 52 weeks"
Attributes: scale between 0 and 100, add 1 if necessary
HHclean: Estimate with step-dummy: starting in 2007.

Try fd_bt

